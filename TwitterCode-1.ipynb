{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "from scipy.misc import imread\n",
    "\n",
    "import matplotlib as mpl\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Authentication:\n",
    "https://beginanalyticsblog.wordpress.com/2018/02/07/twitter-data-analysis-using-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Authenticating my account to access twitterâ€™s API.\n",
    "consumer_key = \"LtTYnj1rtVRNpcxVKnB0QhqmZ\"\n",
    "consumer_secret = \"DOUo6wXpG5RnivUYiR2iTP04qg1wNfcgwsKQf6GE5hru01phyR\"\n",
    "access_token = \"367018485-YIzt7FonHnU292KZWDHBu2QdFpZOQHpDGo6e1kAc\"\n",
    "access_token_secret = \"maJFd0ge7SY3c3tljWxCvzw7SZiOybpaOo1I6aLGocLJj\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the authentication object\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# Setting your access token and secret\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "# Creating the API object while passing in auth information\n",
    "api = tweepy.API(auth) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1493\n"
     ]
    }
   ],
   "source": [
    "# Search term that you want to find\n",
    "query = \"iphone8\"\n",
    "\n",
    "# Language code to select a language\n",
    "language = \"en\"\n",
    "\n",
    "#Extracting tweets from twitter and appending to result object.\n",
    "results = []\n",
    "for tweet in tweepy.Cursor (api.search, q = 'iphone8', lang = \"en\").items(1500): \n",
    "    results.append(tweet)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Cleaning:\n",
    "Store the relevant tweets data in a data frame and remove tweets that have duplicate text. You may find tweets that have the exact same text but have been re-tweeted by different users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extracting elements from the tweet and populating data sets\n",
    "def tweets_df(results):\n",
    "    \n",
    "    id_list = [tweet.id for tweet  in results]\n",
    "    data_set = pd.DataFrame(id_list, columns = [\"id\"])\n",
    "    \n",
    "    #Creating datafarmes for tweet fields.\n",
    "    data_set[\"text\"] = [tweet.text for tweet in results]\n",
    "    data_set[\"created_at\"] = [tweet.created_at for tweet in results]\n",
    "    data_set[\"user_screen_name\"] = [tweet.author.screen_name for tweet in results]\n",
    "    data_set[\"user_followers_count\"] = [tweet.author.followers_count for tweet in results]\n",
    "    data_set[\"user_location\"] = [tweet.author.location for tweet in results]\n",
    "    data_set[\"user_verified\"] = [tweet.author.verified for tweet in results]\n",
    "    data_set[\"user_friends_count\"] = [tweet.author.friends_count for tweet in results]\n",
    "    data_set[\"user_listed_count\"] = [tweet.author.listed_count for tweet in results]\n",
    "    data_set[\"user_favourites_count\"] = [tweet.author.favourites_count for tweet in results]\n",
    "    data_set[\"\"] = [tweet.author.statuses_count for tweet in results]\n",
    "    data_set[\"user_geo_enabled\"] = [tweet.author.geo_enabled for tweet in results]\n",
    "    data_set[\"Hashtags\"] = [tweet.entities.get('hashtags') for tweet in results]\n",
    "    \n",
    "    return data_set\n",
    "\n",
    "\n",
    "#Getting the extracted tweets and removing fields from them.\n",
    "data_set = tweets_df(results)\n",
    "\n",
    "\n",
    "# Remove tweets with duplicate\n",
    "text = data_set[\"text\"]\n",
    "\n",
    "for i in range(0,len(text)):\n",
    "    txt = ' '.join(word for word in text[i] .split() if not word.startswith('https:'))\n",
    "    data_set.set_value(i,'text2', txt)\n",
    "    \n",
    "data_set.drop_duplicates('text2', inplace=True)\n",
    "data_set.reset_index(drop = True, inplace=True)\n",
    "data_set.drop('text', axis = 1, inplace = True)\n",
    "data_set.rename(columns={'text2': 'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set.to_csv('output294.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take this data and mannually label them as bot or no bot and create a file 'outputbot.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Beautiful soup for data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tok = WordPunctTokenizer()\n",
    "\n",
    "data1= pd.read_csv('outputbot.csv')\n",
    "\n",
    "#removing @ mentions\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "#removing links\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    # Remove HTML encoding that has been converted to text\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    #decoding anny UTF-8 encoding and replacing the characters by ?\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    \n",
    "    #cleaning the hash tags and numbers\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    #covering text to lower case\n",
    "    lower_case = letters_only.lower()\n",
    "    # Removing unnecessary white spaces\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "\n",
    "#removing stop words from tweets\n",
    "def msg_transform(message):\n",
    "    #Takes in a string of text, then performs the following:\n",
    "    #1. Remove all punctuation\n",
    "    #2. Remove all stopwords\n",
    "    #3. Returns a list of the cleaned text\n",
    "    # Check characters to see if they are in punctuation\n",
    "    rmv_punc = [char for char in message if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    rmv_punc = ''.join(rmv_punc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in rmv_punc.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "#we clean the tweets and then remove stop words from them and the output the csv file to be used by our Bag of words algorithm.\n",
    "testing = data1['text']\n",
    "test_result = []\n",
    "for t in testing:\n",
    "    test_result.append(tweet_cleaner(t))\n",
    "\n",
    "\n",
    "test_result1 = []\n",
    "for t in test_result:\n",
    "    test_result1.append(msg_transform(t))\n",
    "\n",
    "\n",
    "test_result1\n",
    "my_df = pd.DataFrame(test_result1)\n",
    "my_df.to_csv('bow.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We store the result of cleaned tweets to file. \n",
    "my_df = pd.DataFrame(test_result)\n",
    "my_df.to_csv('clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     0\n",
      "0    all your problems to watch ipl matches can be ...\n",
      "1            is iphone or samsung s worth to buy today\n",
      "2                                       rt iphone cool\n",
      "3                                               iphone\n",
      "4                                        nicchi iphone\n",
      "5    win one of these iphonex iphone just retweet f...\n",
      "6    rt we re excited to announce our newest giveaw...\n",
      "7    paige i m going to get iphone plus red it s pa...\n",
      "8             rt y all see what a iphone can do prom k\n",
      "9    we d love to work with you to get the iphone f...\n",
      "10               y all see what a iphone can do prom k\n",
      "11   rt iphone x space grey apple iphonex ios iphon...\n",
      "12   iphone x space grey apple iphonex ios iphoneph...\n",
      "13   im playing fortnite on my iphone the future is...\n",
      "14   win one of these iphonex iphone just retweet f...\n",
      "15   no filter one shot of this gorgeous east longm...\n",
      "16   yesterday i checked out iphone product red wit...\n",
      "17                                              iphone\n",
      "18   rt top racing games with realistic graphics fo...\n",
      "19   win one of these iphonex iphone just retweet f...\n",
      "20   perfect number for that saturday night out thi...\n",
      "21   make right climate changes now with new life s...\n",
      "22   rt iphone just launched competition retweet fo...\n",
      "23   win one of these iphonex amp iphone just retwe...\n",
      "24   good evening from mexico mexico vacation moon ...\n",
      "25   new personal best clashroyale mobilegames clan...\n",
      "26   rt kuik new personal best clashroyale mobilega...\n",
      "27   rt my first iphone giveaway win the brand new ...\n",
      "28   new personal best clashroyale mobilegames clan...\n",
      "29   win one of these iphonex iphone just retweet f...\n",
      "..                                                 ...\n",
      "412  special edition iphone iphone plus now in indi...\n",
      "413       the new red iphone is now available in india\n",
      "414  win one of these iphonex iphone just retweet f...\n",
      "415  for iphone x s iphone plus case ultra slim hyb...\n",
      "416    special edition iphone iphone plus now in india\n",
      "417  pdca iphone ionic newapp it is pocketpdca that...\n",
      "418  apple contributes a big part of each iphone pl...\n",
      "419                                             iphone\n",
      "420  smartphone wallpaper info black and white patt...\n",
      "421  apple contributes a big part of each iphone se...\n",
      "422  using blackberry only passport se but since wh...\n",
      "423  sorry i ve had to delete the new app guys it s...\n",
      "424  black and white pattern character iphone wallp...\n",
      "425  this special edition product offers customers ...\n",
      "426  win one of these iphonex iphone just retweet f...\n",
      "427  botanical flower iphone case flower botanical ...\n",
      "428  loving hearts bling swarovski crystal iphone a...\n",
      "429  apple s special edition iphone and iphone plus...\n",
      "430  special edition iphone iphone plus available i...\n",
      "431    special edition iphone iphone plus now in india\n",
      "432  swimming with the elnido palawan philippines i...\n",
      "433  rt apple iphone product red iphone plus red ar...\n",
      "434                               iphone good question\n",
      "435                          iphone yes yes you should\n",
      "436  my selfie with g t onairtel g hope to win an i...\n",
      "437  apple iphone product red iphone plus red are a...\n",
      "438  beautiful places cover for iphone s iphone iph...\n",
      "439  iphone silicone phone cover with attractive wh...\n",
      "440  pre order iphone iphone plus in india on to go...\n",
      "441  apple iphone iphone plus product red now avail...\n",
      "\n",
      "[442 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(my_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "https://towardsdatascience.com/another-twitter-sentiment-analysis-bb5b01ebad90\n",
    "https://www.analyticsvidhya.com/blog/2014/11/text-data-cleaning-steps-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
